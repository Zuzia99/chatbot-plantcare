Aby poprawić jakość i kompletność generowanych odpowiedzi, warto dostosować te parametry:​

max_tokens: Określa maksymalną liczbę tokenów w wygenerowanej odpowiedzi. Zwiększenie tej wartości pozwoli modelowi na tworzenie dłuższych i bardziej szczegółowych odpowiedzi.​

temperature: Kontroluje poziom losowości w generowanych odpowiedziach. Niższe wartości (np. 0.2) sprawiają, że odpowiedzi są bardziej przewidywalne, podczas gdy wyższe wartości (np. 0.8) zwiększają kreatywność odpowiedzi.​

top_p: Znany również jako nucleus sampling, określa próg kumulatywnego prawdopodobieństwa tokenów branych pod uwagę podczas generacji. Wartość 0.9 oznacza, że model wybiera spośród tokenów, których łączne prawdopodobieństwo wynosi 90%.

Parametr max_new_tokens określa maksymalną liczbę tokenów, które model wygeneruje dodatkowo, poza tekstem wejściowym. Nie ma jednego "idealnego" zakresu – wszystko zależy od Twojego zastosowania oraz od długości oczekiwanej odpowiedzi. W praktyce możesz eksperymentować z wartościami od około 20 do 300 tokenów. Jeśli potrzebujesz krótkiej, zwięzłej odpowiedzi, warto ustawić tę wartość na dolnej granicy, np. 50–100 tokenów. Jeśli chcesz uzyskać bardziej rozbudowaną odpowiedź, możesz podnieść tę wartość.

Parametry temperature i top_p wpływają na sposób generowania tekstu przez model, czyli na "kreatywność" i "różnorodność" jego odpowiedzi:

Temperature:
Parametr ten kontroluje losowość generowanego tekstu.

Przy niższej wartości (np. 0.2–0.5) model staje się bardziej deterministyczny i spójny – odpowiada w sposób przewidywalny, wybierając najbardziej prawdopodobne tokeny.

Przy wyższej wartości (np. 0.8–1.0 lub więcej) model generuje bardziej zróżnicowane i kreatywne odpowiedzi, ale mogą one być mniej spójne i bardziej "fantazyjne".

top_p (nucleus sampling):
Ten parametr określa próg, według którego model wybiera tokeny do generacji. Model analizuje rozkład prawdopodobieństwa i bierze pod uwagę tylko te tokeny, które razem osiągają sumaryczne prawdopodobieństwo równą lub większą niż wartość top_p.

Przy niższej wartości (np. 0.7–0.8) model wybiera z mniejszej liczby najprawdopodobniejszych tokenów, co daje bardziej "konserwatywne" i spójne odpowiedzi.

Przy wyższej wartości (blisko 1.0) model uwzględnia szerszy zakres tokenów, co zwiększa różnorodność, ale może prowadzić do mniej spójnych wyników.

W skrócie:

max_new_tokens decyduje o długości wygenerowanej odpowiedzi.

Temperature steruje stopniem deterministyczności – im niższa, tym bardziej przewidywalna odpowiedź.

top_p ustala, z jakiej puli tokenów model ma wybierać, co wpływa na zróżnicowanie odpowiedzi.

Mam nadzieję, że to wyjaśnienie pomoże Ci lepiej dostosować parametry generacji tekstu! Czy mogę pomóc w czymś jeszcze?